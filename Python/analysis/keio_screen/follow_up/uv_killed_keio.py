#!/usr/bin/env python3# -*- coding: utf-8 -*-"""Analyse Keio Dead Bacteria experiment results- Follow-up analysis of Keio hit strains that have been UV-irradiated to kill the bacteria and test  whether the bacteria still modify worm behaviour when dead - Does the effect require a biotic interaction between the bacteria and the worm?   Is the behaviour-modifying molecule only released by the bacteria in response to being ingested?4 hours on food - standard syngenta bluelight videosPlease run the following scripts beforehand:1. preprocessing/compile_keio_results.py2. statistical_testing/perform_dead_keio_stats.pyMain feature we are using as an indicator for the rescue: 'speed_50th_bluelight'@author: sm5911@date: 19/11/2021"""#%% IMPORTSimport numpy as npimport pandas as pdimport seaborn as snsfrom tqdm import tqdmfrom pathlib import Pathfrom matplotlib import pyplot as plt# from matplotlib import pyplot as plt# from matplotlib import transforms # patches# from scipy.stats import zscore # levene, ttest_ind, f_oneway, kruskalfrom preprocessing.compile_hydra_data import compile_metadata, process_feature_summariesfrom filter_data.clean_feature_summaries import clean_summary_results# from clustering.hierarchical_clustering import plot_clustermap # plot_barcode_heatmap# from feature_extraction.decomposition.pca import plot_pca # remove_outliers_pca# from feature_extraction.decomposition.tsne import plot_tSNE# from feature_extraction.decomposition.umap import plot_umapfrom time_series.plot_timeseries import plot_timeseries_feature, plot_timeseries # selected_strains_timeseriesfrom time_series.time_series_helper import get_strain_timeseriesfrom visualisation.plotting_helper import sig_asterix, all_in_one_boxplotsfrom write_data.write import write_list_to_filefrom tierpsytools.analysis.statistical_tests import univariate_tests, get_effect_sizesfrom tierpsytools.preprocessing.filter_data import select_feat_setfrom tierpsytools.analysis.statistical_tests import _multitest_correct#%% GLOBALSPROJECT_DIR = "/Volumes/hermes$/Keio_Dead_96WP"SAVE_DIR = "/Users/sm5911/Documents/Keio_Dead"# JSON_PARAMETERS_PATH = "analysis/20211109_parameters_keio_dead.json"CONTROL_STRAIN = 'BW'CONTROL_TREATMENT = 'live'STRAIN_SUBSET = ['wild_type','fepD']FEATURE_SET = ['speed_50th']nan_threshold_row = 0.8nan_threshold_col = 0.2METHOD = 'complete' # 'complete','linkage','average','weighted','centroid'METRIC = 'euclidean' # 'euclidean','cosine','correlation'N_WELLS = 96FPS = 25WINDOW_DICT = {0:(65,75),1:(90,100),               2:(165,175),3:(190,200),               4:(265,275),5:(290,300)}WINDOW_NAME_DICT = {0:"blue light 1", 1: "20-30 seconds after blue light 1",                    2:"blue light 2", 3: "20-30 seconds after blue light 2",                    4:"blue light 3", 5: "20-30 seconds after blue light 3"}#%% FUNCTIONSdef uv_killing_stats(metadata,                     features,                     group_by='treatment',                     control='BW-live',                     save_dir=None,                     feature_set=None,                     pvalue_threshold=0.05,                     fdr_method='fdr_bh'):        # check case-sensitivity    assert len(metadata[group_by].unique()) == len(metadata[group_by].str.upper().unique())        if feature_set is not None:        feature_set = [feature_set] if isinstance(feature_set, str) else feature_set        assert isinstance(feature_set, list)        assert(all(f in features.columns for f in feature_set))    else:        feature_set = features.columns.tolist()            features = features[feature_set].reindex(metadata.index)    # print mean sample size    sample_size = metadata.groupby(group_by).count()    print("Mean sample size of %s: %d" % (group_by, int(sample_size[sample_size.columns[-1]].mean())))    n = len(metadata[group_by].unique())            fset = []    if n > 2:           # Perform ANOVA - is there variation among strains at each window?        anova_path = Path(save_dir) / 'ANOVA' / 'ANOVA_results.csv'        anova_path.parent.mkdir(parents=True, exist_ok=True)        stats, pvals, reject = univariate_tests(X=features,                                                 y=metadata[group_by],                                                 control=control,                                                 test='ANOVA',                                                comparison_type='multiclass',                                                multitest_correction=fdr_method,                                                alpha=pvalue_threshold,                                                n_permutation_test=None)        # get effect sizes        effect_sizes = get_effect_sizes(X=features,                                        y=metadata[group_by],                                        control=control,                                        effect_type=None,                                        linked_test='ANOVA')        # compile + save results        test_results = pd.concat([stats, effect_sizes, pvals, reject], axis=1)        test_results.columns = ['stats','effect_size','pvals','reject']             test_results['significance'] = sig_asterix(test_results['pvals'])        test_results = test_results.sort_values(by=['pvals'], ascending=True) # rank by p-value        test_results.to_csv(anova_path, header=True, index=True)        # use reject mask to find significant feature set        fset = pvals.loc[reject['ANOVA']].sort_values(by='ANOVA', ascending=True).index.to_list()        if len(fset) > 0:            print("%d significant features found by ANOVA by '%s' (P<%.2f, %s)" %\                  (len(fset), group_by, pvalue_threshold, fdr_method))            anova_sigfeats_path = anova_path.parent / 'ANOVA_sigfeats.txt'            write_list_to_file(fset, anova_sigfeats_path)                 # Perform t-tests    stats_t, pvals_t, reject_t = univariate_tests(X=features,                                                  y=metadata[group_by],                                                  control=control,                                                  test='t-test',                                                  comparison_type='binary_each_group',                                                  multitest_correction=fdr_method,                                                  alpha=pvalue_threshold)        effect_sizes_t = get_effect_sizes(X=features,                                      y=metadata[group_by],                                      control=control,                                      linked_test='t-test')        stats_t.columns = ['stats_' + str(c) for c in stats_t.columns]    pvals_t.columns = ['pvals_' + str(c) for c in pvals_t.columns]    reject_t.columns = ['reject_' + str(c) for c in reject_t.columns]    effect_sizes_t.columns = ['effect_size_' + str(c) for c in effect_sizes_t.columns]    ttest_results = pd.concat([stats_t, pvals_t, reject_t, effect_sizes_t], axis=1)        # save results    ttest_path = Path(save_dir) / 't-test' / 't-test_results.csv'    ttest_path.parent.mkdir(exist_ok=True, parents=True)    ttest_results.to_csv(ttest_path, header=True, index=True)        nsig = sum(reject_t.sum(axis=1) > 0)    print("%d significant features between any %s vs %s (t-test, P<%.2f, %s)" %\          (nsig, group_by, control, pvalue_threshold, fdr_method))    return# def compare_keio_dead(features, #                       metadata, #                       group_by, #                       control, #                       save_dir, #                       stats_dir,#                       feature_set=None,#                       pvalue_threshold=0.05):#     """ Compare live vs dead Keio single-gene deletion mutants with the respective wild-type #         BW25113 control strain, and look to see if whether UV-killing the bacteria affects their #         influence on worm behaviour.        #         - Boxplots for each feature, comparing each strain vs control, for live and dead bacteria#           separately#         - Boxplots for each feature, comparing live vs dead for each strain        #         Inputs#         ------#         features, metadata : pd.DataFrame#             Matching features summaries and metadata        #         args : Object #             Python object with the following attributes:#             - drop_size_features : bool#             - norm_features_only : bool#             - percentile_to_use : str#             - remove_outliers : bool#             - omit_strains : list#             - control_dict : dict#             - n_top_feats : int#             - tierpsy_top_feats_dir (if n_top_feats) : str#             - test : str#             - pval_threshold : float#             - fdr_method : str#             - n_sig_features : int#     """#     assert set(features.index) == set(metadata.index)#     treatment_list = metadata[group_by].unique().tolist()#     assert control in treatment_list#     treatment_list = [control] + [s for s in sorted(treatment_list) if s != control]        #     assert not features.isna().any().any()       #     # load t-test results#     ttest_path = Path(stats_dir) / 't-test_results.csv'#     ttest_df = pd.read_csv(ttest_path, index_col=0)#     pvals = ttest_df[[c for c in ttest_df.columns if "pvals" in c]]#     pvals.columns = [c.split('pvals_')[-1] for c in pvals.columns]    #     # dates = list(metadata['date_yyyymmdd'].unique())#     # date_lut = dict(zip(dates, sns.color_palette('plasma', len(dates))))    #     if feature_set is not None:#         assert isinstance(feature_set, list) and all(f in features.columns for f in feature_set)    #     plot_df = metadata.join(features[feature_set])    #     # boxplots for each feature, comparing each treatment to control#     for feature in tqdm(features[feature_set].columns):        #         plt.close('all')#         fig, ax = plt.subplots(figsize=(14,8))#         ax = sns.boxplot(x=group_by, #                          y=feature, #                          order=treatment_list, #                          data=plot_df,#                          palette='plasma')            #         ax = sns.swarmplot(x=group_by, #                            y=feature, #                            order=treatment_list,#                            hue='date_yyyymmdd', #                            dodge=False,#                            data=plot_df,#                            palette='Greys', #                            alpha=0.7, #                            size=4)        #         # ax.set_xlabel('Treatment', fontsize=12, labelpad=15)#         ax.set_ylabel(feature.replace('_',' '), fontsize=12, labelpad=15)#         ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')    #         # add custom legend#         # patch_list = []#         # for s, c in zip(strain_list, sns.color_palette('Set3', n_colors=len(strain_list))):#         #     patch = patches.Patch(color=c, label=s)#         #     patch_list.append(patch)#         # plt.tight_layout(rect=[0.04, 0, 0.84, 0.96])#         # ax.legend(handles=patch_list, labels=strain_list, loc=(1.02, 0.5),\#         #            borderaxespad=0.4, frameon=False, fontsize=15)                #         # scale plot to omit outliers (>2.5*IQR from mean)#         if scale_outliers_box:#             grouped_strain = plot_df.groupby(group_by)#             y_bar = grouped_strain[feature].median() # median is less skewed by outliers#             # Computing IQR#             Q1 = grouped_strain[feature].quantile(0.25)#             Q3 = grouped_strain[feature].quantile(0.75)#             IQR = Q3 - Q1#             plt.ylim(min(y_bar) - 2 * max(IQR), max(y_bar) + 2.5 * max(IQR))            #         # annotate p-values#         for ii, treatment in enumerate(treatment_list[1:]):#             p = pvals.loc[feature, treatment]#             text = ax.get_xticklabels()[ii+1]#             assert text.get_text() == treatment#             p_text = 'P < 0.001' if p < 0.001 else 'P = %.3f' % p#             p_text = '{}\n'.format(sig_asterix([p])[0]) + p_text#             #y = (y_bar[strain] + 2 * IQR[strain]) if scale_outliers_box else plot_df[feature].max()#             #h = (max(IQR) / 10) if scale_outliers_box else (y - plot_df[feature].min()) / 50#             trans = transforms.blended_transform_factory(ax.transData, ax.transAxes)#             # plt.plot([ii-.2, ii-.2, ii+.2, ii+.2], [y+h, y+2*h, y+2*h, y+h], #             #          lw=1.5, c='k', transform=trans)#             ax.text(ii+1, 1.02, p_text, fontsize=6, ha='center', va='bottom', transform=trans)                #         save_path = Path(save_dir) / 'boxplots' / '{}.pdf'.format(feature)#         save_path.parent.mkdir(exist_ok=True, parents=True)#         plt.savefig(save_path)                        #     # # load ANOVA results for significant features#     # test_path = stats_dir / 'ANOVA_results.csv'#     # anova_table = pd.read_csv(test_path, header=0, index_col=0)#     # pvals_heatmap = anova_table.loc[control_clustered_features, 'pvals']#     # pvals_heatmap.name = 'P < {}'.format(args.pval_threshold)    #     # assert all(f in featZ.columns for f in pvals_heatmap.index)            #     # # Plot heatmap (averaged for each sample)#     # if len(metadata[group_by].unique()) < 250:#     #     print("\nPlotting barcode heatmap")#     #     heatmap_path = Path(save_dir) / 'heatmaps' / 'full_heatmap.pdf'#     #     plot_barcode_heatmap(featZ=featZ[control_clustered_features], #     #                          meta=metadata, #     #                          group_by=group_by, #     #                          pvalues_series=pvals_heatmap,#     #                          p_value_threshold=pvalue_threshold,#     #                          selected_feats=None, # fset if len(fset) > 0 else None#     #                          saveto=heatmap_path,#     #                          figsize=[20,30],#     #                          sns_colour_palette="Pastel1",#     #                          label_size=20,#     #                          sub_adj={'bottom':0.01,'left':0.15,'top':0.95,'right':0.92})                #     return#%% MAINif __name__ == "__main__":        aux_dir = Path(PROJECT_DIR) / 'AuxiliaryFiles'    res_dir = Path(PROJECT_DIR) / 'Results'        metadata_path_local = Path(SAVE_DIR) / 'metadata.csv'    features_path_local = Path(SAVE_DIR) / 'features.csv'        if not metadata_path_local.exists() and not features_path_local.exists():        metadata, metadata_path = compile_metadata(aux_dir,                                                    n_wells=N_WELLS,                                                   imaging_dates=None,                                                   add_well_annotations=True,                                                   from_source_plate=True)                features, metadata = process_feature_summaries(metadata_path,                                                        results_dir=res_dir,                                                        compile_day_summaries=True,                                                        imaging_dates=None,                                                        align_bluelight=False,                                                        window_summaries=True,                                                       n_wells=N_WELLS)        # Clean results - Remove bad well data + features with too many NaNs/zero std + impute NaNs        features, metadata = clean_summary_results(features,                                                    metadata,                                                   feature_columns=None,                                                   nan_threshold_row=nan_threshold_row,                                                   nan_threshold_col=nan_threshold_col,                                                   max_value_cap=1e15,                                                   imputeNaN=True,                                                   min_nskel_per_video=None,                                                   min_nskel_sum=None,                                                   drop_size_related_feats=False,                                                   norm_feats_only=False)                # save clean metadata and features        metadata.to_csv(metadata_path_local, index=False)        features.to_csv(features_path_local, index=False)            else:        metadata = pd.read_csv(metadata_path_local, header=0, index_col=None, dtype={'comments':str})        features = pd.read_csv(features_path_local, header=0, index_col=None)    assert not features.isna().sum(axis=1).any()    assert not (features.std(axis=1) == 0).any()        # load feature set    if FEATURE_SET is not None:        # subset for selected feature set (and remove path curvature features)        if isinstance(FEATURE_SET, int) and FEATURE_SET in [8,16,256]:            features = select_feat_set(features, 'tierpsy_{}'.format(FEATURE_SET), append_bluelight=True)            features = features[[f for f in features.columns if 'path_curvature' not in f]]        elif isinstance(FEATURE_SET, list) or isinstance(FEATURE_SET, set):            assert all(f in features.columns for f in FEATURE_SET)            features = features[FEATURE_SET].copy()    feature_list = features.columns.tolist()    # subset metadata results for bluelight videos only     bluelight_videos = [i for i in metadata['imgstore_name'] if 'bluelight' in i]    metadata = metadata[metadata['imgstore_name'].isin(bluelight_videos)]        # add combined treatment column    metadata['is_dead'] = ['dead' if i else 'live' for i in metadata['dead']]    metadata['treatment'] = metadata[['gene_name','is_dead']].agg('-'.join, axis=1)    control = CONTROL_STRAIN + '-' + CONTROL_TREATMENT    metadata['window'] = metadata['window'].astype(int)    window_list = list(metadata['window'].unique())        strains2exclude = [s for s in metadata['gene_name'].unique() if s.startswith('trp') or s.startswith('ent')]    metadata = metadata[~metadata['gene_name'].isin(strains2exclude)]    strain_list = sorted(metadata['gene_name'].unique())    for window in tqdm(window_list):        meta_window = metadata[metadata['window']==window]        feat_window = features.reindex(meta_window.index)        stats_dir = Path(SAVE_DIR) / 'Stats' / WINDOW_NAME_DICT[window]        plot_dir = Path(SAVE_DIR) / 'Plots' / WINDOW_NAME_DICT[window]        uv_killing_stats(meta_window,                         feat_window,                         group_by='treatment',                         control=control,                         save_dir=stats_dir,                         feature_set=FEATURE_SET,                         pvalue_threshold=0.05,                         fdr_method='fdr_bh')                    all_in_one_boxplots(meta_window,                            feat_window,                            group_by='gene_name',                            hue='is_dead',                            control='BW',                            control_hue='live',                            order=None,                            hue_order=['live','dead'],                            colour_dict=None,                            save_dir=plot_dir / 'all-in-one',                            ttest_path=stats_dir / 't-test' / 't-test_results.csv',                            feature_set=feature_list,                            pvalue_threshold=0.05,                            sigasterix=True,                            fontsize=35,                            figsize=(20,15),                            ylim_minmax=(-200,350),                            vline_boxpos=None,                            legend=False,                            subplots_adjust={'bottom':0.1,'top':0.95,'left':0.1,'right':0.95})            # correct p-values for multiple comparisons: live vs dead for each strain    pvalues_dict = {}    for strain in strain_list:        meta_strain = meta_window[meta_window['gene_name']==strain]        feat_strain = feat_window.reindex(meta_strain.index)                uv_killing_stats(meta_strain,                         feat_strain,                         group_by='is_dead',                         control='live',                         save_dir=stats_dir / 'live_vs_dead_for_each_strain' / strain,                         feature_set=FEATURE_SET,                         pvalue_threshold=0.05,                         fdr_method='fdr_bh')                # read p-values for each strain and correct for multiple comparisons (fdr_bh)        pvals_df = pd.read_csv(stats_dir / 'live_vs_dead_for_each_strain' / strain / 't-test' / 't-test_results.csv',                               index_col=0)        pvalues_dict[strain] = pvals_df.loc['speed_50th','pvals_dead']        reject, corrected_pvals = _multitest_correct(pd.Series(list(pvalues_dict.values())),                                                  multitest_method='fdr_bh', fdr=0.05)    pvalues_dict = dict(zip(strain_list, corrected_pvals))    pvals = pd.DataFrame.from_dict(pvalues_dict, orient='index', columns=['pvals'])    save_path = stats_dir / 'live_vs_dead_for_each_strain' / 't-test_corrected' / 't-test_results.csv'    save_path.parent.mkdir(exist_ok=True, parents=True)    pvals.to_csv(save_path)            # compare_keio_dead(features,     #                   metadata,     #                   group_by='treatment',    #                   control=control,    #                   save_dir=get_save_dir(args) / 'Plots' / args.fdr_method,     #                   stats_dir=get_save_dir(args) / 'Stats' / args.fdr_method,    #                   feature_set=feature_set)        # if STRAIN_SUBSET is not None:    #     metadata = metadata[metadata['gene_name'].isin(STRAIN_SUBSET)]     # ### Hierarchical Clustering    # control_strain_meta = metadata[metadata[group_by]==control]    # control_strain_feat = features.reindex(control_strain_meta.index)    # # Z-normalise control data    # control_strain_featZ = control_strain_feat.apply(zscore, axis=0)    # # drop features with zero standard deviation after normalising    # control_strain_featZ = feat_filter_std(control_strain_featZ, threshold=0)            # ### Control clustermap        # print("\nPlotting clustermap (date) for '%s' control" % control)    # n_features = features.shape[1]    # # control data is clustered and feature order is stored and applied to full data        # control_clustermap_path = Path(save_dir) / 'heatmaps' / 'control_date_clustermap.pdf'    # cg = plot_clustermap(control_strain_featZ, control_strain_meta,    #                      group_by='date_yyyymmdd',    #                      method=METHOD,     #                      metric=METRIC,    #                      figsize=[20,6],    #                      sub_adj={'bottom':0.05,'left':0,'top':1,'right':0.85},    #                      saveto=control_clustermap_path,    #                      label_size=15,    #                      show_xlabels=False)    # # control clustermap with labels    # if n_features <= 256:    #     control_clustermap_path = Path(save_dir) / 'heatmaps' / 'control_date_clustermap_label.pdf'    #     cg = plot_clustermap(control_strain_featZ, control_strain_meta,    #                          group_by='date_yyyymmdd',    #                          method=METHOD,     #                          metric=METRIC,    #                          figsize=[20,10],    #                          sub_adj={'bottom':0.7,'left':0,'top':1,'right':0.85},    #                          saveto=control_clustermap_path,    #                          label_size=(15,15),    #                          show_xlabels=True)        # #col_linkage = cg.dendrogram_col.calculated_linkage    # _ = np.array(control_strain_featZ.columns)[cg.dendrogram_col.reordered_ind]    # ### Full clustermap         # # Z-normalise data for all strains    # featZ = features.apply(zscore, axis=0)                        # # Save z-normalised values    # z_stats_path = stats_dir / 'z-normalised_values.csv'    # z_stats = featZ.join(metadata[group_by]).groupby(by=group_by).mean().T    # z_stats.columns = ['z-mean_' + v for v in z_stats.columns.to_list()]    # z_stats.to_csv(z_stats_path, header=True, index=None)        # # drop features with zero standard deviation after normalising    # featZ = feat_filter_std(featZ, threshold=0)        # # Clustermap of full data       # print("Plotting all treatments clustermap")        # full_clustermap_path = Path(save_dir) / 'heatmaps' / 'full_clustermap.pdf'    # fg = plot_clustermap(featZ, metadata,     #                      group_by=group_by,    #                      row_colours=None,    #                      method=METHOD,     #                      metric=METRIC,    #                      figsize=[20,30],    #                      sub_adj={'bottom':0.01,'left':0,'top':1,'right':0.95},    #                      saveto=full_clustermap_path,    #                      label_size=20,    #                      show_xlabels=False)    # if n_features <= 256:    #     full_clustermap_path = Path(save_dir) / 'heatmaps' / 'full_clustermap_label.pdf'    #     fg = plot_clustermap(featZ, metadata,     #                          group_by=group_by,    #                          row_colours=None,    #                          method=METHOD,     #                          metric=METRIC,    #                          figsize=[20,40],    #                          sub_adj={'bottom':0.18,'left':0,'top':1,'right':0.95},    #                          saveto=full_clustermap_path,    #                          label_size=20,    #                          show_xlabels=True)        # # clustered feature order for all strains    # _ = np.array(featZ.columns)[fg.dendrogram_col.reordered_ind]    # ### Principal Components Analysis    # pca_dir = Path(save_dir) / 'PCA'        # _ = plot_pca(featZ, metadata,     #              group_by=group_by,     #              control=control,    #              var_subset=treatment_list,     #              saveDir=pca_dir,    #              PCs_to_keep=10,    #              n_feats2print=10,    #              kde=False,    #              sns_colour_palette="Paired",    #              n_dims=2,    #              label_size=10,    #              sub_adj={'bottom':0.13,'left':0.13,'top':0.95,'right':0.82},    #              legend_loc=[1.02,0.03],    #              hypercolor=False,    #              s=30,    #              alpha=0.7)    # mean_sample_size = round(metadata.groupby(group_by)['well_name'].count().mean())    # print("Mean sample size per treatment: %d" % mean_sample_size)        # # t-distributed Stochastic Neighbour Embedding    # tsne_dir = Path(save_dir) / 'tSNE'    # perplexities = [mean_sample_size] # NB: should be roughly equal to group size        # _ = plot_tSNE(featZ, metadata,    #               group_by=group_by,    #               var_subset=treatment_list,    #               saveDir=tsne_dir,    #               perplexities=perplexities,    #               figsize=[10,10],    #               label_size=10,    #               sns_colour_palette="Paired",    #               s=100,    #               alpha=0.7)    # # Uniform Manifold Projection    # umap_dir = Path(save_dir) / 'UMAP'    # n_neighbours = [mean_sample_size] # NB: should be roughly equal to group size    # min_dist = 0.1 # Minimum distance parameter        # _ = plot_umap(featZ, metadata,    #               group_by=group_by,    #               var_subset=treatment_list,    #               saveDir=umap_dir,    #               n_neighbours=n_neighbours,    #               min_dist=min_dist,    #               figsize=[8,8],    #               label_size=10,    #               sns_colour_palette="Paired",    #               s=30,    #               alpha=0.7)    metadata = metadata[metadata['window']==0]    ### Timeseries     # timeseries plots of speed for each treatment vs control    keys = ['BW','fepD']    strain_list = sorted([s for s in list(metadata['treatment'].unique()) if s.split('-')[0] in keys])    plot_timeseries_feature(metadata,                            project_dir=Path(PROJECT_DIR),                            save_dir=Path(SAVE_DIR) / 'timeseries-speed',                            group_by='treatment',                            control=control,                            groups_list=strain_list,                            feature='speed',                            n_wells=96,                            bluelight_stim_type='bluelight',                            video_length_seconds=360,                            bluelight_timepoints_seconds=[(60, 70),(160, 170),(260, 270)],                            smoothing=10,                            fps=FPS,                            #ylim_minmax=(-20,330)                            )    # bespoke timeseries        rescue_list = ['fepD','fes','fepB','nuoC','sdhD','atpB']    for strain in tqdm(rescue_list):        groups = ['BW-live', strain + '-live', strain + '-dead']        print("Plotting timeseries speed for %s" % strain)                bluelight_frames = [(i*FPS, j*FPS) for (i, j) in [(60, 70),(160, 170),(260, 270)]]        feature = 'speed'        save_dir = Path(SAVE_DIR) / 'timeseries-speed' / 'rescues'        ts_plot_dir = save_dir / 'Plots' / strain        ts_plot_dir.mkdir(exist_ok=True, parents=True)        save_path = ts_plot_dir / 'speed_bluelight.pdf'                plt.close('all')        fig, ax = plt.subplots(figsize=(15,6), dpi=300)        col_dict = dict(zip(groups, sns.color_palette('tab10', len(groups))))        for group in groups:                        # get control timeseries            group_ts = get_strain_timeseries(metadata,                                             project_dir=Path(PROJECT_DIR),                                             strain=group,                                             group_by='treatment',                                             feature_list=[feature],                                             save_dir=save_dir,                                             n_wells=N_WELLS,                                             verbose=True)                        ax = plot_timeseries(df=group_ts,                                 feature=feature,                                 error=True,                                 max_n_frames=360*FPS,                                  smoothing=10*FPS,                                  ax=ax,                                 bluelight_frames=bluelight_frames,                                 colour=col_dict[group])        plt.ylim(-20, 300)        xticks = np.linspace(0, 360*FPS, int(360/60)+1)        ax.set_xticks(xticks)        ax.set_xticklabels([str(int(x/FPS/60)) for x in xticks])           ax.set_xlabel('Time (minutes)', fontsize=20, labelpad=10)        ylab = feature.replace('_50th'," (µm s$^{-1}$)")        ax.set_ylabel(ylab, fontsize=20, labelpad=10)        ax.legend(groups, fontsize=12, frameon=False, loc='best', handletextpad=1)        plt.subplots_adjust(left=0.1, top=0.98, bottom=0.15, right=0.98)        # save plot        print("Saving to: %s" % save_path)        plt.savefig(save_path)