#!/usr/bin/env python3# -*- coding: utf-8 -*-"""Analyse Keio Dead Bacteria experiment results- Follow-up analysis of Keio hit strains that have been UV-irradiated to kill the bacteria and test  whether the bacteria still modify worm behaviour when dead - Does the effect require a biotic interaction between the bacteria and the worm?   Is the behaviour-modifying molecule only released by the bacteria in response to being ingested?Please run the following scripts beforehand:1. preprocessing/compile_keio_results.py2. statistical_testing/perform_dead_keio_stats.pyMain feature we are using as an indicator for the rescue: 'motion_mode_paused_fraction_bluelight'@author: sm5911@date: 19/11/2021"""#%% IMPORTSimport argparseimport numpy as npimport pandas as pdimport seaborn as snsfrom time import timefrom tqdm import tqdmfrom pathlib import Pathfrom matplotlib import pyplot as pltfrom matplotlib import transforms # patchesfrom scipy.stats import zscore # levene, ttest_ind, f_oneway, kruskalfrom read_data.paths import get_save_dirfrom read_data.read import load_json #load_topfeats#from analysis.control_variation import control_variationfrom clustering.hierarchical_clustering import plot_clustermap # plot_barcode_heatmapfrom feature_extraction.decomposition.pca import plot_pca # remove_outliers_pcafrom feature_extraction.decomposition.tsne import plot_tSNEfrom feature_extraction.decomposition.umap import plot_umapfrom time_series.plot_timeseries import selected_strains_timeseries, plot_timeseries_featurefrom visualisation.plotting_helper import sig_asterixfrom tierpsytools.preprocessing.filter_data import select_feat_set, feat_filter_std#%% GLOBALSJSON_PARAMETERS_PATH = "analysis/20211109_parameters_keio_dead.json"CONTROL_STRAIN = 'wild_type'CONTROL_TREATMENT = 'live'STRAIN_SUBSET = ['wild_type','fepD']#feature = ['motion_mode_forward_fraction_bluelight']scale_outliers_box = TrueMETHOD = 'complete' # 'complete','linkage','average','weighted','centroid'METRIC = 'euclidean' # 'euclidean','cosine','correlation'N_WELLS = 6FPS = 25feature_set=['motion_mode_forward_fraction_prestim',             'motion_mode_forward_fraction_bluelight',             'motion_mode_forward_fraction_poststim',             'speed_50th_prestim',             'speed_50th_bluelight',             'speed_50th_poststim',             'curvature_midbody_norm_abs_50th_prestim',             'curvature_midbody_norm_abs_50th_bluelight',             'curvature_midbody_norm_abs_50th_poststim']    #%% FUNCTIONSdef compare_keio_dead(features,                       metadata,                       group_by,                       control,                       save_dir,                       stats_dir,                      feature_set=None,                      pvalue_threshold=0.05):    """ Compare live vs dead Keio single-gene deletion mutants with the respective wild-type         BW25113 control strain, and look to see if whether UV-killing the bacteria affects their         influence on worm behaviour.                - Boxplots for each feature, comparing each strain vs control, for live and dead bacteria          separately        - Boxplots for each feature, comparing live vs dead for each strain                Inputs        ------        features, metadata : pd.DataFrame            Matching features summaries and metadata                args : Object             Python object with the following attributes:            - drop_size_features : bool            - norm_features_only : bool            - percentile_to_use : str            - remove_outliers : bool            - omit_strains : list            - control_dict : dict            - n_top_feats : int            - tierpsy_top_feats_dir (if n_top_feats) : str            - test : str            - pval_threshold : float            - fdr_method : str            - n_sig_features : int    """    assert set(features.index) == set(metadata.index)    treatment_list = metadata[group_by].unique().tolist()    assert control in treatment_list    treatment_list = [control] + [s for s in sorted(treatment_list) if s != control]            assert not features.isna().any().any()           # load t-test results    ttest_path = Path(stats_dir) / 't-test_results.csv'    ttest_df = pd.read_csv(ttest_path, index_col=0)    pvals = ttest_df[[c for c in ttest_df.columns if "pvals" in c]]    pvals.columns = [c.split('pvals_')[-1] for c in pvals.columns]        # dates = list(metadata['date_yyyymmdd'].unique())    # date_lut = dict(zip(dates, sns.color_palette('plasma', len(dates))))        if feature_set is not None:        assert isinstance(feature_set, list) and all(f in features.columns for f in feature_set)        plot_df = metadata.join(features[feature_set])        # boxplots for each feature, comparing each treatment to control    for feature in tqdm(features[feature_set].columns):                plt.close('all')        fig, ax = plt.subplots(figsize=(14,8))        ax = sns.boxplot(x=group_by,                          y=feature,                          order=treatment_list,                          data=plot_df,                         palette='plasma')                    ax = sns.swarmplot(x=group_by,                            y=feature,                            order=treatment_list,                           hue='date_yyyymmdd',                            dodge=False,                           data=plot_df,                           palette='Greys',                            alpha=0.7,                            size=4)                # ax.set_xlabel('Treatment', fontsize=12, labelpad=15)        ax.set_ylabel(feature.replace('_',' '), fontsize=12, labelpad=15)        ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')            # add custom legend        # patch_list = []        # for s, c in zip(strain_list, sns.color_palette('Set3', n_colors=len(strain_list))):        #     patch = patches.Patch(color=c, label=s)        #     patch_list.append(patch)        # plt.tight_layout(rect=[0.04, 0, 0.84, 0.96])        # ax.legend(handles=patch_list, labels=strain_list, loc=(1.02, 0.5),\        #            borderaxespad=0.4, frameon=False, fontsize=15)                        # scale plot to omit outliers (>2.5*IQR from mean)        if scale_outliers_box:            grouped_strain = plot_df.groupby(group_by)            y_bar = grouped_strain[feature].median() # median is less skewed by outliers            # Computing IQR            Q1 = grouped_strain[feature].quantile(0.25)            Q3 = grouped_strain[feature].quantile(0.75)            IQR = Q3 - Q1            plt.ylim(min(y_bar) - 2 * max(IQR), max(y_bar) + 2.5 * max(IQR))                    # annotate p-values        for ii, treatment in enumerate(treatment_list[1:]):            p = pvals.loc[feature, treatment]            text = ax.get_xticklabels()[ii+1]            assert text.get_text() == treatment            p_text = 'P < 0.001' if p < 0.001 else 'P = %.3f' % p            p_text = '{}\n'.format(sig_asterix([p])[0]) + p_text            #y = (y_bar[strain] + 2 * IQR[strain]) if scale_outliers_box else plot_df[feature].max()            #h = (max(IQR) / 10) if scale_outliers_box else (y - plot_df[feature].min()) / 50            trans = transforms.blended_transform_factory(ax.transData, ax.transAxes)            # plt.plot([ii-.2, ii-.2, ii+.2, ii+.2], [y+h, y+2*h, y+2*h, y+h],             #          lw=1.5, c='k', transform=trans)            ax.text(ii+1, 1.02, p_text, fontsize=6, ha='center', va='bottom', transform=trans)                        save_path = Path(save_dir) / 'boxplots' / '{}.pdf'.format(feature)        save_path.parent.mkdir(exist_ok=True, parents=True)        plt.savefig(save_path)                        ### Hierarchical Clustering    control_strain_meta = metadata[metadata[group_by]==control]    control_strain_feat = features.reindex(control_strain_meta.index)    # Z-normalise control data    control_strain_featZ = control_strain_feat.apply(zscore, axis=0)    # drop features with zero standard deviation after normalising    control_strain_featZ = feat_filter_std(control_strain_featZ, threshold=0)            ### Control clustermap        print("\nPlotting clustermap (date) for '%s' control" % control)    n_features = features.shape[1]    # control data is clustered and feature order is stored and applied to full data        control_clustermap_path = Path(save_dir) / 'heatmaps' / 'control_date_clustermap.pdf'    cg = plot_clustermap(control_strain_featZ, control_strain_meta,                         group_by='date_yyyymmdd',                         method=METHOD,                          metric=METRIC,                         figsize=[20,6],                         sub_adj={'bottom':0.05,'left':0,'top':1,'right':0.85},                         saveto=control_clustermap_path,                         label_size=15,                         show_xlabels=False)    # control clustermap with labels    if n_features <= 256:        control_clustermap_path = Path(save_dir) / 'heatmaps' / 'control_date_clustermap_label.pdf'        cg = plot_clustermap(control_strain_featZ, control_strain_meta,                             group_by='date_yyyymmdd',                             method=METHOD,                              metric=METRIC,                             figsize=[20,10],                             sub_adj={'bottom':0.7,'left':0,'top':1,'right':0.85},                             saveto=control_clustermap_path,                             label_size=(15,15),                             show_xlabels=True)        #col_linkage = cg.dendrogram_col.calculated_linkage    _ = np.array(control_strain_featZ.columns)[cg.dendrogram_col.reordered_ind]    ### Full clustermap         # Z-normalise data for all strains    featZ = features.apply(zscore, axis=0)                        # Save z-normalised values    z_stats_path = stats_dir / 'z-normalised_values.csv'    z_stats = featZ.join(metadata[group_by]).groupby(by=group_by).mean().T    z_stats.columns = ['z-mean_' + v for v in z_stats.columns.to_list()]    z_stats.to_csv(z_stats_path, header=True, index=None)        # drop features with zero standard deviation after normalising    featZ = feat_filter_std(featZ, threshold=0)        # Clustermap of full data       print("Plotting all treatments clustermap")        full_clustermap_path = Path(save_dir) / 'heatmaps' / 'full_clustermap.pdf'    fg = plot_clustermap(featZ, metadata,                          group_by=group_by,                         row_colours=None,                         method=METHOD,                          metric=METRIC,                         figsize=[20,30],                         sub_adj={'bottom':0.01,'left':0,'top':1,'right':0.95},                         saveto=full_clustermap_path,                         label_size=20,                         show_xlabels=False)    if n_features <= 256:        full_clustermap_path = Path(save_dir) / 'heatmaps' / 'full_clustermap_label.pdf'        fg = plot_clustermap(featZ, metadata,                              group_by=group_by,                             row_colours=None,                             method=METHOD,                              metric=METRIC,                             figsize=[20,40],                             sub_adj={'bottom':0.18,'left':0,'top':1,'right':0.95},                             saveto=full_clustermap_path,                             label_size=20,                             show_xlabels=True)        # clustered feature order for all strains    _ = np.array(featZ.columns)[fg.dendrogram_col.reordered_ind]        # # load ANOVA results for significant features    # test_path = stats_dir / 'ANOVA_results.csv'    # anova_table = pd.read_csv(test_path, header=0, index_col=0)    # pvals_heatmap = anova_table.loc[control_clustered_features, 'pvals']    # pvals_heatmap.name = 'P < {}'.format(args.pval_threshold)        # assert all(f in featZ.columns for f in pvals_heatmap.index)                # # Plot heatmap (averaged for each sample)    # if len(metadata[group_by].unique()) < 250:    #     print("\nPlotting barcode heatmap")    #     heatmap_path = Path(save_dir) / 'heatmaps' / 'full_heatmap.pdf'    #     plot_barcode_heatmap(featZ=featZ[control_clustered_features],     #                          meta=metadata,     #                          group_by=group_by,     #                          pvalues_series=pvals_heatmap,    #                          p_value_threshold=pvalue_threshold,    #                          selected_feats=None, # fset if len(fset) > 0 else None    #                          saveto=heatmap_path,    #                          figsize=[20,30],    #                          sns_colour_palette="Pastel1",    #                          label_size=20,    #                          sub_adj={'bottom':0.01,'left':0.15,'top':0.95,'right':0.92})            ### Principal Components Analysis    pca_dir = Path(save_dir) / 'PCA'        _ = plot_pca(featZ, metadata,                  group_by=group_by,                  control=control,                 var_subset=treatment_list,                  saveDir=pca_dir,                 PCs_to_keep=10,                 n_feats2print=10,                 kde=False,                 sns_colour_palette="Paired",                 n_dims=2,                 label_size=10,                 sub_adj={'bottom':0.13,'left':0.13,'top':0.95,'right':0.82},                 legend_loc=[1.02,0.03],                 hypercolor=False,                 s=30,                 alpha=0.7)    mean_sample_size = round(metadata.groupby(group_by)['well_name'].count().mean())    print("Mean sample size per treatment: %d" % mean_sample_size)        # t-distributed Stochastic Neighbour Embedding    tsne_dir = Path(save_dir) / 'tSNE'    perplexities = [mean_sample_size] # NB: should be roughly equal to group size        _ = plot_tSNE(featZ, metadata,                  group_by=group_by,                  var_subset=treatment_list,                  saveDir=tsne_dir,                  perplexities=perplexities,                  figsize=[10,10],                  label_size=10,                  sns_colour_palette="Paired",                  s=100,                  alpha=0.7)    # Uniform Manifold Projection    umap_dir = Path(save_dir) / 'UMAP'    n_neighbours = [mean_sample_size] # NB: should be roughly equal to group size    min_dist = 0.1 # Minimum distance parameter        _ = plot_umap(featZ, metadata,                  group_by=group_by,                  var_subset=treatment_list,                  saveDir=umap_dir,                  n_neighbours=n_neighbours,                  min_dist=min_dist,                  figsize=[8,8],                  label_size=10,                  sns_colour_palette="Paired",                  s=30,                  alpha=0.7)            return#%% MAINif __name__ == "__main__":    tic = time()    parser = argparse.ArgumentParser(description="Read clean features and etadata and find 'hit' \                                                  Keio knockout strains that alter worm behaviour")    parser.add_argument('-j', '--json', help="Path to JSON parameters file",                         default=JSON_PARAMETERS_PATH, type=str)    parser.add_argument('--features_path', help="Path to feature summaries file",                         default=None, type=str)    parser.add_argument('--metadata_path', help="Path to metadata file",                         default=None, type=str)    args = parser.parse_args()    args = load_json(args.json)        FEATURES_PATH = Path(args.save_dir) / 'features.csv'    METADATA_PATH = Path(args.save_dir) / 'metadata.csv'            # Read clean feature summaries + metadata    print("Loading metadata and feature summary results...")    features = pd.read_csv(FEATURES_PATH)    metadata = pd.read_csv(METADATA_PATH, dtype={'comments':str, 'source_plate_id':str})    # Subset for desired imaging dates       if args.dates is not None:        assert type(args.dates) == list        metadata = metadata.loc[metadata['date_yyyymmdd'].astype(str).isin(args.dates)]        features = features.reindex(metadata.index)    # Load Tierpsy feature set + subset (columns) for selected features only    if args.n_top_feats is not None:        features = select_feat_set(features, 'tierpsy_{}'.format(args.n_top_feats), append_bluelight=True)        features = features[[f for f in features.columns if 'path_curvature' not in f]]    # add combined treatment column    metadata['is_dead'] = ['dead' if i else 'live' for i in metadata['dead']]    metadata['treatment'] = metadata[['gene_name','is_dead']].agg('-'.join, axis=1)    control = CONTROL_STRAIN + '-' + CONTROL_TREATMENT    compare_keio_dead(features,                       metadata,                       group_by='treatment',                      control=control,                      save_dir=get_save_dir(args) / 'Plots' / args.fdr_method,                       stats_dir=get_save_dir(args) / 'Stats' / args.fdr_method,                      feature_set=feature_set)        if STRAIN_SUBSET is not None:        metadata = metadata[metadata['gene_name'].isin(STRAIN_SUBSET)]            selected_strains_timeseries(metadata,                                project_dir=Path(args.project_dir),                                 save_dir=Path(args.save_dir) / 'timeseries',                                 strain_list=None,                                group_by='treatment',                                control=control,                                n_wells=96,                                bluelight_stim_type='bluelight',                                video_length_seconds=360,                                bluelight_timepoints_seconds=[(60, 70),(160, 170),(260, 270)],                                motion_modes=['forwards','paused','backwards'],                                smoothing=10,                                fps=FPS)        # timeseries plots of speed for each treatment vs control    plot_timeseries_feature(metadata,                            project_dir=Path(args.project_dir),                            save_dir=Path(args.save_dir) / 'timeseries-speed',                            group_by='treatment',                            control=control,                            groups_list=None,                            feature='speed',                            n_wells=96,                            bluelight_stim_type='bluelight',                            video_length_seconds=360,                            bluelight_timepoints_seconds=[(60, 70),(160, 170),(260, 270)],                            smoothing=10,                            fps=FPS,                            ylim_minmax=(0,200))        toc = time()    print("\nDone in %.1f seconds (%.1f minutes)" % (toc - tic, (toc - tic) / 60))  